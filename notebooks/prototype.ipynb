{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06175294-e62c-43a2-9b80-6c9056eeeea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55d8f0e6-c3bd-46be-b31c-db152fd0ca0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63577dc4-d00b-46b0-9460-997456cc1e90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"/mnt/c/Users/nikol/Projects/gpt2test/data/tinyshakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915f4810-0853-4f8e-b7b2-6449f83a4697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First Citizen:\\n',\n",
       " 'Before we proceed any further, hear me speak.\\n',\n",
       " '\\n',\n",
       " 'All:\\n',\n",
       " 'Speak, speak.\\n',\n",
       " '\\n',\n",
       " 'First Citizen:\\n',\n",
       " 'You are all resolved rather to die than to famish?\\n',\n",
       " '\\n',\n",
       " 'All:\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = DATA_PATH.open(\"r\").readlines()\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0072b2-095b-4c21-86e9-588b3afd57c7",
   "metadata": {},
   "source": [
    "### Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5689eb97-f438-49ac-b88d-e40d31886d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get tokenizer for gpt2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee366d41-5503-4f1f-930e-ce04224056e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model, use language model head\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c7fd29c-d721-4ec0-b563-74fb5db73a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize text and return pytorch tensor\n",
    "text = \"Be or not to be, thats a question.\"\n",
    "tokenizer_out = tokenizer(text, return_tensors='pt')\n",
    "input_ids = tokenizer_out[\"input_ids\"]\n",
    "attention_mask = tokenizer_out[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc924568-7227-4d16-9652-ecea1ee4da24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Use inference\n",
    "output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=1000, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3518018-2055-4878-afd1-e24ba6d18c27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be or not to be, thats a question. It's like how people get paid? It's like you can never get paid, right? If you were to buy a car in your 20s to 20s, do you think there would be any downside to that? It's the same as buying a house. How does that affect you personally? No. That's the thing. Because you think once you're dead, you don't want to live. So you go, 'Who are you?' People say, 'When you are dead, you don't want to live.' We're like, 'What is your life to living?' You never answer those questions, so you don't know. You feel you've told people that for one reason or another, but I think that's the key thing is that, when you've passed away, how do you know? People don't know how to live out their lives or how to live back up with, to be honest, their kids, their grandkidsâ€”all of that is a massive responsibility. Advertisement So I guess that's my point. I really don't want to be there any more. Yeah. Advertisement Have you ever talked about a family-wise philosophy of life? I mean, it's a big deal, really, because you feel so disconnected from the outside world. You kind of kind of feel like people's life isn't that good and you're not living that life. So I think that is part of it. It's pretty obvious that not everything has to be like the outside world and there's just a huge amount of anxiety and depression. You mentioned that some of your friends feel like life has the opposite of the outside world. What were they like about living with that? We all had a family experience too, but we were not in it for the money stuff and stuff like that. Oh, right. It totally had a certain life of it. But, you know, I think that can be a different thing for someone like me. Advertisement It definitely didn't have that. Yeah. Absolutely, no. And I had one experience where I had to go out on foot to see where I was when I died and say, 'I'm going' and you go back at the foot and say, 'What time am I walking over to when I was sitting there?' And I didn't understand it. I felt so disconnected from the world. This interview has been edited and condensed.\n"
     ]
    }
   ],
   "source": [
    "# Decode tokens\n",
    "out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\" \".join([line.strip() for line in out.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268acb1-98a0-4f78-85f7-719f5e409280",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prototype fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd9a9da-85a6-4762-8653-6dd867c4eaad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyy92/anaconda3/envs/gpt2-shakespeare/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7798a542-bb09-4531-b86d-6bc823a2f5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingParams:\n",
    "    pretrained_model_name_or_path: str\n",
    "    \n",
    "    data_path: Path\n",
    "    checkpoints_dir: Path\n",
    "\n",
    "    num_train_epochs: int\n",
    "    batch_size: int\n",
    "    learning_rate: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e41c4a-7ba8-421f-bd51-8e95e1621fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_finetune_shakespeare = TrainingParams(\n",
    "    pretrained_model_name_or_path=\"gpt2\",\n",
    "    data_path=Path(\"/mnt/c/Users/nikol/Projects/gpt2test/data/tinyshakespeare.txt\"),\n",
    "    checkpoints_dir=Path(\"./checkpoints\"),\n",
    "    num_train_epochs=5,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94392258-ec37-4e3b-a4c2-ed5937619b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, params: TrainingParams):\n",
    "        self._params = params\n",
    "        \n",
    "        self._tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=params.pretrained_model_name_or_path)\n",
    "        \n",
    "        # Init dataset\n",
    "        self._dataset = TextDataset(tokenizer=self._tokenizer, file_path=params.data_path, block_size=128)\n",
    "                \n",
    "        # Init data loaders\n",
    "        self._train_dataloader = DataLoader(self._dataset, batch_size=params.batch_size)\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self) -> GPT2Tokenizer:\n",
    "        return self._tokenizer\n",
    "    \n",
    "    @property\n",
    "    def num_batches(self) -> int:\n",
    "        return len(self._train_dataloader)\n",
    "    \n",
    "    @property\n",
    "    def data_size(self) -> int:\n",
    "        return len(self._dataset)\n",
    "    \n",
    "    def train(self, model: GPT2LMHeadModel) -> tuple[GPT2LMHeadModel, int, Path]:\n",
    "        # Init optimizer\n",
    "        optimizer = AdamW(model.parameters(), lr=self._params.learning_rate)\n",
    "        \n",
    "        # Init scheduler\n",
    "        num_epochs = self._params.num_train_epochs\n",
    "        num_training_steps = num_epochs * len(self._train_dataloader)\n",
    "        progress_bar = tqdm(range(num_training_steps))\n",
    "        lr_scheduler = get_scheduler(\n",
    "            name=\"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "        \n",
    "        # Load to device\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Train loop\n",
    "        model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in self._train_dataloader:\n",
    "                # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                batch = batch.to(device)\n",
    "                outputs = model(batch, labels=batch)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "            \n",
    "            print(f\"Epoch: {epoch} loss: {loss.cpu().detach().numpy()}\")\n",
    "            torch.save(model.state_dict(), self._params.checkpoints_dir / f\"gpt2s-{epoch}.pt\",)\n",
    "        \n",
    "        last_ckpt = self._params.checkpoints_dir / f\"gpt2s-{epoch}.pt\"\n",
    "        return model, loss.cpu().detach().numpy(), last_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab9f94f-78fc-44b7-846b-62698a7859cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    pretrained_model_name_or_path=gpt_finetune_shakespeare.pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5af544-389f-4284-b94d-fcda0c9b8a36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2640 330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyy92/anaconda3/envs/gpt2-shakespeare/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(params=gpt_finetune_shakespeare)\n",
    "print(trainer.data_size, trainer.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e781ee3-f474-4df3-9886-d803fc5d9118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                           | 330/1650 [01:17<04:50,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 3.5530896186828613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 660/1650 [02:30<03:35,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 3.406376600265503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 990/1650 [03:44<02:26,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 loss: 3.3193750381469727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 1320/1650 [04:57<01:13,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 loss: 3.27297306060791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1650/1650 [06:11<00:00,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 loss: 3.266934633255005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1650/1650 [06:12<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "model, loss, last_ckpt = trainer.train(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42883c25-312c-4989-a6e5-fb91f6f50d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test fine-tuned model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=gpt_finetune_shakespeare.pretrained_model_name_or_path)\n",
    "# Tokenize text and return pytorch tensor\n",
    "text = \"Be or not to be, thats a question.\"\n",
    "tokenizer_out = tokenizer(text, return_tensors='pt')\n",
    "input_ids = tokenizer_out[\"input_ids\"]\n",
    "attention_mask = tokenizer_out[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cad37706-887b-4797-97f3-c38c732ebbac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_fine_tuned = GPT2LMHeadModel.from_pretrained(\n",
    "    pretrained_model_name_or_path=gpt_finetune_shakespeare.pretrained_model_name_or_path,\n",
    "    state_dict=torch.load(last_ckpt),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2cbb380-9274-4d26-a106-02fb838e59ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be or not to be, thats a question. I do think that's your business, and you should be my husband. How was I made your master? MESSAGE: If you make any sort of acquaintance with me, I must tell you myself, 'tis my husband; he shall not lie with you in such a matter, for I'll think him good for it. HENRY BOLINGBROKE: No, you know me well enough to believe this: it was not done till my father, George Edward Blount, came to England with you, for my father's sake. Away with me: I have to hear him speak. 'Tis his way to the king, though he is too young to live, for he is like a good horse and a noble hatter; nor so with me; but, as the king says,'s not far off' The Duke of York will send him a present, for he is the elder and more gentle Richard's sister; not so with myself: not as you mean, but as a good man with a wife to be married; an Englishwoman is very mean and sweet indeed, and will live; therefore for if she should live to-morrow To-day, she could not come a fortnight without your welcome; now She would, though we be friends, to the king. KING RICHARD III: I am in good terms with thee: I am as well as you know That Henry, as Clarence, Edward, and Margaret are: 'tis this that, being noble, I will help you to a fair marriage for my first husband. KING RICHARD III: It is a noble, that's it, Is it, in view of what comes in from my noble selfs, and of that love, which is so sweet and lovely As our father, Clarence, Edward, and Margaret--of mine, Because my wife as my brother is most virtuous and kindest brother and heir of such a love, And so hath as much courtesy for his sake My own marriage, wherein I am so gentle and gentle And kind as his brother's, And so fair and fair as his, as both my two love The king and my wife, which I love not, yet cannot, My husband: he must. KING RICHARD III: If you must, you'll make no way nor leave no way; So I'll have an heir of mine in my brother. KING RICHARD III: Come away, madam, and follow me to the king: he needs your help. When I had sent forth these letters, What is your answer? MESSAGE: Heaven be the king, my lord. KING RICHARD III: There needs be an appeal from us. MESSAGE: We'll have them in dispatch, and make such a request. KING RICHARD III: I say he must see me, and hear from me: for, being a gentleman of your own will, I am more than willing to make it a request. KING RICHARD III: What, then, doth not the king call you a gentleman of his own? MESSAGE: The Duke of Norfolk he cannot answer not this: when I think of his wife, She hath my name, and the queen's name is Earl of Warwick, Who, as your noble cousin in the house, Shall prove to be my mistress, though in the house This is not her name: I'll let my good lord hear the thing on the present account: I think I know him well enough: It did not be your great good and lord Richard's pleasure, To hear my name and my good Earl Warwick, A gentlemanly man with his wife; thus I shall. KING RICHARD III: And then have the most sweet hearted and gracious king To say, 'Good lady, do do let me do answer. Now Warwick is gone. Why, what shall we do? Tell me not, I pray you; for Warwick's mother's head Is gone; I am no more to hear of it. Now 'tis your lordship's death: Warwick lives but a day, that he can come to speak of it; and I think his grief may come upon it. KING RICHARD III: I tell thee what, I say, 'tis such an issue As shall not be subject to our gentle cousin Warwick, Which would be a matter to ourselves\n"
     ]
    }
   ],
   "source": [
    "output = model_fine_tuned.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=1000,\n",
    "    do_sample=True,\n",
    ")\n",
    "out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\" \".join([line.strip() for line in out.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e63f2-040c-4bba-9684-4d03c0703c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
